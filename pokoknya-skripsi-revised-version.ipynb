{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9864692,"sourceType":"datasetVersion","datasetId":6054854},{"sourceId":9951025,"sourceType":"datasetVersion","datasetId":6119642},{"sourceId":159174,"sourceType":"modelInstanceVersion","modelInstanceId":135317,"modelId":158045},{"sourceId":170990,"sourceType":"modelInstanceVersion","modelInstanceId":145510,"modelId":168080},{"sourceId":172894,"sourceType":"modelInstanceVersion","modelInstanceId":147183,"modelId":169706},{"sourceId":215055,"sourceType":"modelInstanceVersion","modelInstanceId":183343,"modelId":205536},{"sourceId":220456,"sourceType":"modelInstanceVersion","modelInstanceId":188020,"modelId":210063},{"sourceId":222655,"sourceType":"modelInstanceVersion","modelInstanceId":189942,"modelId":211939},{"sourceId":248346,"sourceType":"modelInstanceVersion","modelInstanceId":212259,"modelId":233932},{"sourceId":248805,"sourceType":"modelInstanceVersion","modelInstanceId":212669,"modelId":234317},{"sourceId":252810,"sourceType":"modelInstanceVersion","modelInstanceId":216156,"modelId":237897},{"sourceId":252900,"sourceType":"modelInstanceVersion","modelInstanceId":216236,"modelId":237972},{"sourceId":257428,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":220020,"modelId":241778}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/lucianor/pokoknya-skripsi-revised-version?scriptVersionId=232804403\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport os\nimport tensorflow as tf\nfrom PIL import Image\nimport shutil\nimport random","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Notice Update, all Trainset was resampled and privately uploaded in KaggleDatasets","metadata":{}},{"cell_type":"markdown","source":"# Label's","metadata":{}},{"cell_type":"code","source":"import json\nfile_path = '/kaggle/input/processed-imagenet-dataset-224-splitted-train-test/Labels.json'\nwith open(file_path, 'r') as file:\n    labels = json.load(file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:10:52.84343Z","iopub.execute_input":"2025-03-17T19:10:52.843974Z","iopub.status.idle":"2025-03-17T19:10:52.862808Z","shell.execute_reply.started":"2025-03-17T19:10:52.843944Z","shell.execute_reply":"2025-03-17T19:10:52.861979Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_label = {key:[i,labels[key]] for i,key in enumerate(sorted(labels))}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:10:52.863924Z","iopub.execute_input":"2025-03-17T19:10:52.864216Z","iopub.status.idle":"2025-03-17T19:10:52.869268Z","shell.execute_reply.started":"2025-03-17T19:10:52.864189Z","shell.execute_reply":"2025-03-17T19:10:52.868254Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:10:52.870317Z","iopub.execute_input":"2025-03-17T19:10:52.870612Z","iopub.status.idle":"2025-03-17T19:10:52.888044Z","shell.execute_reply.started":"2025-03-17T19:10:52.870583Z","shell.execute_reply":"2025-03-17T19:10:52.887101Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"picked_class_folders = ['n01440764', 'n01443537','n01531178','n01537544','n01582220','n01592084','n01601694','n01667778','n01695060','n01698640','n01751748','n01756291','n01770081','n01770393','n01773157','n01774750','n01795545','n01798484','n01806143','n01818515','n01820546','n01824575','n01828970','n01833805','n01843383','n01860187','n01910747','n01914609','n01924916','n01930112','n01950731','n01955084','n01986214','n02006656','n02007558','n02011460','n02013706','n02018207','n02027492','n02037110','n02051845','n02077923']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:10:52.889136Z","iopub.execute_input":"2025-03-17T19:10:52.889437Z","iopub.status.idle":"2025-03-17T19:10:52.896754Z","shell.execute_reply.started":"2025-03-17T19:10:52.889409Z","shell.execute_reply":"2025-03-17T19:10:52.895929Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\ndef load_new_image_class_train(start_class, end_class, picked_class=False):\n    print(\"Load_new_image_class_train\")\n    main_directory = \"/kaggle/input/processed-imagenet-dataset-192-splitted-train-test/train\"\n    images = []\n    labels = []\n    if(picked_class):\n        picked_class_folders = ['n01440764', 'n01443537','n01531178','n01537544','n01582220','n01592084','n01601694','n01667778','n01695060','n01698640','n01751748','n01756291','n01770081','n01770393','n01773157','n01774750','n01795545','n01798484','n01806143','n01818515','n01820546','n01824575','n01828970','n01833805','n01843383','n01860187','n01910747','n01914609','n01924916','n01930112','n01950731','n01955084','n01986214','n02006656','n02007558','n02011460','n02013706','n02018207','n02027492','n02037110','n02051845','n02077923']\n        class_folders = picked_class_folders[start_class:end_class]\n    else:\n        class_folders = sorted(os.listdir(main_directory))[start_class:end_class]\n    for label, class_folder in enumerate(class_folders):\n        print(label,class_folder)\n        class_path = os.path.join(main_directory, class_folder)\n        \n        # Check if it's indeed a directory\n        if os.path.isdir(class_path):\n            counter = 0\n            for image_name in os.listdir(class_path):\n                image_path = os.path.join(class_path, image_name)\n                \n                # Open and resize the image to 150x150\n                img = Image.open(image_path)\n                if(img.mode!='RGB'):\n                    img = np.array(img.convert(\"RGB\"))\n                    counter = counter+1\n                    \n                # Convert image to numpy array and flatten to a 1D array of pixel values\n                img_data = np.array(img)\n                # img_data_with_label = np.concatenate((img_data_with_label, label_data), axis=-1)\n                images.append(img_data)\n                labels.append(label+start_class)\n            print(f\"New Train Image {class_folder} has {counter} non-RGB\")\n    images = np.array(images,dtype=float)\n    images = images/255.0\n    labels = np.array(labels,dtype=int)\n    labels = to_categorical(labels, end_class)\n    return images,labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:10:52.8999Z","iopub.execute_input":"2025-03-17T19:10:52.900248Z","iopub.status.idle":"2025-03-17T19:10:52.939092Z","shell.execute_reply.started":"2025-03-17T19:10:52.900219Z","shell.execute_reply":"2025-03-17T19:10:52.938078Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_new_image_class_test(end_class,picked_class=False):\n    main_directory = \"/kaggle/input/processed-imagenet-dataset-192-splitted-train-test/test\"\n    images = []\n    labels = []\n    print(\"Load_new_image_class_test\")\n    if(picked_class):\n        picked_class_folders = ['n01440764', 'n01443537','n01531178','n01537544','n01582220','n01592084','n01601694','n01667778','n01695060','n01698640','n01751748','n01756291','n01770081','n01770393','n01773157','n01774750','n01795545','n01798484','n01806143','n01818515','n01820546','n01824575','n01828970','n01833805','n01843383','n01860187','n01910747','n01914609','n01924916','n01930112','n01950731','n01955084','n01986214','n02006656','n02007558','n02011460','n02013706','n02018207','n02027492','n02037110','n02051845','n02077923']\n        class_folders = picked_class_folders[:end_class]\n    else:\n        class_folders = sorted(os.listdir(main_directory))[:end_class]\n    for label, class_folder in enumerate(class_folders):\n        print(label,class_folder)\n        class_path = os.path.join(main_directory, class_folder)\n        \n        # Check if it's indeed a directory\n        if os.path.isdir(class_path):\n            counter = 0\n            for image_name in os.listdir(class_path):\n                image_path = os.path.join(class_path, image_name)\n                \n                # Open and resize the image to 150x150\n                img = Image.open(image_path)\n                if(img.mode!='RGB'):\n                    img = np.array(img.convert(\"RGB\"))\n                    counter = counter+1\n                    \n                # Convert image to numpy array and flatten to a 1D array of pixel values\n                img_data = np.array(img)\n                # img_data_with_label = np.concatenate((img_data_with_label, label_data), axis=-1)\n                images.append(img_data)\n                labels.append(label)\n            print(f\"New Test Image {class_folder} has {counter} non-RGB\")\n    images = np.array(images,dtype=float)\n    images = images/255.0\n    labels = np.array(labels,dtype=int)\n    labels = to_categorical(labels, end_class)\n    return images,labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:10:52.940322Z","iopub.execute_input":"2025-03-17T19:10:52.940634Z","iopub.status.idle":"2025-03-17T19:10:52.949115Z","shell.execute_reply.started":"2025-03-17T19:10:52.940606Z","shell.execute_reply":"2025-03-17T19:10:52.948141Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_new_image_class_validation(end_class,picked_class=False):\n    main_directory = \"/kaggle/input/processed-imagenet-dataset-224-splitted-train-test/val\"\n    images = []\n    labels = []\n    print(\"Load Validation\")\n    if(picked_class):\n        picked_class_folders = ['n01440764', 'n01443537','n01531178','n01537544','n01582220','n01592084','n01601694','n01667778','n01695060','n01698640','n01751748','n01756291','n01770081','n01770393','n01773157','n01774750','n01795545','n01798484','n01806143','n01818515','n01820546','n01824575','n01828970','n01833805','n01843383','n01860187','n01910747','n01914609','n01924916','n01930112','n01950731','n01955084','n01986214','n02006656','n02007558','n02011460','n02013706','n02018207','n02027492','n02037110','n02051845','n02077923']\n        class_folders = picked_class_folders[:end_class]\n    else:\n        class_folders = sorted(os.listdir(main_directory))[:end_class]\n    for label, class_folder in enumerate(class_folders):\n        print(label,class_folder)\n        class_path = os.path.join(main_directory, class_folder)\n        \n        # Check if it's indeed a directory\n        if os.path.isdir(class_path):\n            counter = 0\n            for image_name in os.listdir(class_path):\n                image_path = os.path.join(class_path, image_name)\n                \n                # Open and resize the image to 150x150\n                img = Image.open(image_path)\n                img = img.resize((192,192))\n                if(img.mode!='RGB'):\n                    img = np.array(img.convert(\"RGB\"))\n                    counter = counter+1\n                    \n                # Convert image to numpy array and flatten to a 1D array of pixel values\n                img_data = np.array(img)\n                # img_data_with_label = np.concatenate((img_data_with_label, label_data), axis=-1)\n                images.append(img_data)\n                labels.append(label)\n            print(f\"New Validation Image {class_folder} has {counter} non-RGB\")\n    images = np.array(images,dtype=float)\n    images = images/255.0\n    labels = np.array(labels,dtype=int)\n    labels = to_categorical(labels, end_class)\n    return images,labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:10:52.950611Z","iopub.execute_input":"2025-03-17T19:10:52.950975Z","iopub.status.idle":"2025-03-17T19:10:52.964642Z","shell.execute_reply.started":"2025-03-17T19:10:52.950937Z","shell.execute_reply":"2025-03-17T19:10:52.963699Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_buffer_images_class_train(start_class,K_data,step,picked_class=False):\n    # Define the path to the main directory\n    main_directory = \"/kaggle/input/processed-imagenet-dataset-192-splitted-train-test/train\"\n    print(\"Buffer!\")\n    # Initialize lists to store data and labels\n    recall_images = []  # Shape will be (6500, 150, 150)\n    recall_labels = []  # Shape will be (6500,), extracting the single label value\n    num_samples = round(K_data/start_class)\n    # Get the list of class folders, sorted alphabetically\n    if(picked_class):\n        picked_class_folders = ['n01440764', 'n01443537','n01531178','n01537544','n01582220','n01592084','n01601694','n01667778','n01695060','n01698640','n01751748','n01756291','n01770081','n01770393','n01773157','n01774750','n01795545','n01798484','n01806143','n01818515','n01820546','n01824575','n01828970','n01833805','n01843383','n01860187','n01910747','n01914609','n01924916','n01930112','n01950731','n01955084','n01986214','n02006656','n02007558','n02011460','n02013706','n02018207','n02027492','n02037110','n02051845','n02077923']\n        class_folders = picked_class_folders[:start_class]\n    else:\n        class_folders = sorted(os.listdir(main_directory))[:start_class]\n\n    # Iterate over each class folder\n    for label, class_folder in enumerate(class_folders):\n        print(label, class_folder)\n        class_path = os.path.join(main_directory, class_folder)\n        \n        # Check if it's indeed a directory\n        if os.path.isdir(class_path):\n            print('OK')\n            # Get list of all images in the class folder\n            all_images = os.listdir(class_path)\n            \n            # Randomly select 20% of the images\n            selected_images = random.sample(all_images, num_samples)\n        \n            counter = 0\n            for image_name in selected_images:\n                image_path = os.path.join(class_path, image_name)\n                \n                # Open and resize the image to 150x150\n                img = Image.open(image_path)\n                while img.mode != 'RGB':\n                    image_name_temp = random.sample(all_images,1)\n                    img = Image.open(os.path.join(class_path,image_name_temp[0]))\n                    while image_name_temp[0] not in selected_images and img.mode!='RGB':\n                        image_name_temp = random.sample(all_images,1)\n                        img = Image.open(os.path.join(class_path,image_name_temp[0]))\n                        counter = counter + 1\n                    \n                # Convert image to numpy array\n                img_data = np.array(img)\n                recall_images.append(img_data)\n                recall_labels.append(label)\n            \n            print(f\"{counter} recall_images converted to RGB in class {class_folder}\")\n        print('Done Image split')\n    \n    # Convert lists to numpy arrays\n    recall_images = np.array(recall_images, dtype=float)\n    recall_labels = np.array(recall_labels, dtype=int)\n\n    recall_images = recall_images/255.0\n    recall_labels = to_categorical(recall_labels,start_class+step)\n\n    return recall_images, recall_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:10:52.965939Z","iopub.execute_input":"2025-03-17T19:10:52.966251Z","iopub.status.idle":"2025-03-17T19:10:52.979782Z","shell.execute_reply.started":"2025-03-17T19:10:52.966223Z","shell.execute_reply":"2025-03-17T19:10:52.978957Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def merge_data_buffer_extended(buffer_data_x,extend_data_x,buffer_data_y,extend_data_y):\n    merge_data_x = np.concatenate([buffer_data_x, extend_data_x])\n    merge_data_y = np.concatenate([buffer_data_y,extend_data_y])\n    return merge_data_x, merge_data_y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:10:52.98085Z","iopub.execute_input":"2025-03-17T19:10:52.981143Z","iopub.status.idle":"2025-03-17T19:10:52.994728Z","shell.execute_reply.started":"2025-03-17T19:10:52.981115Z","shell.execute_reply":"2025-03-17T19:10:52.993702Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def loading_buffer_ext_train_test_val(start_class, end_class,K_data,step,picked_class=False):\n    print(\"load buffer\")\n    buffer_train, buffer_labels = load_buffer_images_class_train(start_class,K_data,step,picked_class)\n    new_train, new_labels = load_new_image_class_train(start_class,end_class,picked_class)\n    extended_train, extended_labels = merge_data_buffer_extended(buffer_train,new_train,buffer_labels,new_labels)\n    del buffer_train,buffer_labels,new_train,new_labels\n    train_dataset = tf.data.Dataset.from_tensor_slices((extended_train, extended_labels))\n    del extended_train, extended_labels \n\n    images_test, labels_test = load_new_image_class_test(end_class,picked_class)\n    test_dataset = tf.data.Dataset.from_tensor_slices((images_test, labels_test))\n    del images_test, labels_test\n    images_val, labels_val = load_new_image_class_validation(end_class,picked_class)\n    validation_dataset = tf.data.Dataset.from_tensor_slices((images_val, labels_val))\n    del images_val, labels_val\n    return train_dataset, test_dataset, validation_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:10:52.995814Z","iopub.execute_input":"2025-03-17T19:10:52.996079Z","iopub.status.idle":"2025-03-17T19:10:53.005505Z","shell.execute_reply.started":"2025-03-17T19:10:52.996055Z","shell.execute_reply":"2025-03-17T19:10:53.004587Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\ndef load_new_image_class_train_review(start_class, end_class,picked_class=False):\n    print(\"Load_new_image_class_train\")\n    main_directory = \"/kaggle/input/processed-imagenet-dataset-192-splitted-train-test/train\"\n    images = []\n    labels = []\n    if(picked_class):\n        picked_class_folders = ['n01440764', 'n01443537','n01531178','n01537544','n01582220','n01592084','n01601694','n01667778','n01695060','n01698640','n01751748','n01756291','n01770081','n01770393','n01773157','n01774750','n01795545','n01798484','n01806143','n01818515','n01820546','n01824575','n01828970','n01833805','n01843383','n01860187','n01910747','n01914609','n01924916','n01930112','n01950731','n01955084','n01986214','n02006656','n02007558','n02011460','n02013706','n02018207','n02027492','n02037110','n02051845','n02077923']\n        class_folders = picked_class_folders[start_class:end_class]\n    else:\n        class_folders = sorted(os.listdir(main_directory))[start_class:end_class]\n    \n    for label, class_folder in enumerate(class_folders):\n        print(label,class_folder)\n        class_path = os.path.join(main_directory, class_folder)\n        \n        # Check if it's indeed a directory\n        if os.path.isdir(class_path):\n            counter = 0\n            for image_name in os.listdir(class_path):\n                image_path = os.path.join(class_path, image_name)\n                \n                # Open and resize the image to 150x150\n                img = Image.open(image_path)\n                if(img.mode!='RGB'):\n                    img = np.array(img.convert(\"RGB\"))\n                    counter = counter+1\n                    \n                # Convert image to numpy array and flatten to a 1D array of pixel values\n                img_data = np.array(img)\n                # img_data_with_label = np.concatenate((img_data_with_label, label_data), axis=-1)\n                images.append(img_data)\n                labels.append(label)\n    images = np.array(images,dtype=float)\n    images = images/255.0\n    labels = np.array(labels,dtype=int)\n    labels = to_categorical(labels, end_class-start_class)\n    return images,labels\n\ndef load_new_image_class_test_review(start_class, end_class,picked_class=False):\n    main_directory = \"/kaggle/input/processed-imagenet-dataset-192-splitted-train-test/test\"\n    images = []\n    labels = []\n    print(\"Load_new_image_class_test\")\n    if(picked_class):\n        picked_class_folders = ['n01440764', 'n01443537','n01531178','n01537544','n01582220','n01592084','n01601694','n01667778','n01695060','n01698640','n01751748','n01756291','n01770081','n01770393','n01773157','n01774750','n01795545','n01798484','n01806143','n01818515','n01820546','n01824575','n01828970','n01833805','n01843383','n01860187','n01910747','n01914609','n01924916','n01930112','n01950731','n01955084','n01986214','n02006656','n02007558','n02011460','n02013706','n02018207','n02027492','n02037110','n02051845','n02077923']\n        class_folders = picked_class_folders[start_class:end_class]\n    else:\n        class_folders = sorted(os.listdir(main_directory))[start_class:end_class]\n    for label, class_folder in enumerate(class_folders):\n        print(label,class_folder)\n        class_path = os.path.join(main_directory, class_folder)\n        \n        # Check if it's indeed a directory\n        if os.path.isdir(class_path):\n            counter = 0\n            for image_name in os.listdir(class_path):\n                image_path = os.path.join(class_path, image_name)\n                \n                # Open and resize the image to 150x150\n                img = Image.open(image_path)\n                if(img.mode!='RGB'):\n                    img = np.array(img.convert(\"RGB\"))\n                    counter = counter+1\n                    \n                # Convert image to numpy array and flatten to a 1D array of pixel values\n                img_data = np.array(img)\n                # img_data_with_label = np.concatenate((img_data_with_label, label_data), axis=-1)\n                images.append(img_data)\n                labels.append(label)\n    images = np.array(images,dtype=float)\n    images = images/255.0\n    labels = np.array(labels,dtype=int)\n    labels = to_categorical(labels, end_class-start_class)\n    return images,labels\n\ndef load_new_image_class_validation_review(start_class,end_class,picked_class=False):\n    main_directory = \"/kaggle/input/processed-imagenet-dataset-224-splitted-train-test/val\"\n    images = []\n    labels = []\n    if(picked_class):\n        picked_class_folders = ['n01440764', 'n01443537','n01531178','n01537544','n01582220','n01592084','n01601694','n01667778','n01695060','n01698640','n01751748','n01756291','n01770081','n01770393','n01773157','n01774750','n01795545','n01798484','n01806143','n01818515','n01820546','n01824575','n01828970','n01833805','n01843383','n01860187','n01910747','n01914609','n01924916','n01930112','n01950731','n01955084','n01986214','n02006656','n02007558','n02011460','n02013706','n02018207','n02027492','n02037110','n02051845','n02077923']\n        class_folders = picked_class_folders[start_class:end_class]\n    else:\n        class_folders = sorted(os.listdir(main_directory))[start_class:end_class]\n    for label, class_folder in enumerate(class_folders):\n        print(label,class_folder)\n        class_path = os.path.join(main_directory, class_folder)\n        \n        # Check if it's indeed a directory\n        if os.path.isdir(class_path):\n            counter = 0\n            for image_name in os.listdir(class_path):\n                image_path = os.path.join(class_path, image_name)\n                \n                # Open and resize the image to 150x150\n                img = Image.open(image_path)\n                img = img.resize((192,192))\n                if(img.mode!='RGB'):\n                    img = np.array(img.convert(\"RGB\"))\n                    counter = counter+1\n                    \n                # Convert image to numpy array and flatten to a 1D array of pixel values\n                img_data = np.array(img)\n                # img_data_with_label = np.concatenate((img_data_with_label, label_data), axis=-1)\n                images.append(img_data)\n                labels.append(label)\n    images = np.array(images,dtype=float)\n    images = images/255.0\n    labels = np.array(labels,dtype=int)\n    labels = to_categorical(labels, end_class-start_class)\n    return images,labels\n\ndef loading_review_ext_train_test_val(start_class, end_class,picked_class=False):\n    new_train, new_labels = load_new_image_class_train_review(start_class,end_class,picked_class)\n    train_dataset = tf.data.Dataset.from_tensor_slices((new_train, new_labels))\n    del new_train, new_labels\n\n    images_test, labels_test = load_new_image_class_test_review(start_class, end_class,picked_class)\n    test_dataset = tf.data.Dataset.from_tensor_slices((images_test, labels_test))\n    del images_test, labels_test\n    images_val, labels_val = load_new_image_class_validation_review(start_class,end_class,picked_class)\n    validation_dataset = tf.data.Dataset.from_tensor_slices((images_val, labels_val))\n    del images_val, labels_val\n    return train_dataset, test_dataset, validation_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:10:53.007017Z","iopub.execute_input":"2025-03-17T19:10:53.007286Z","iopub.status.idle":"2025-03-17T19:10:53.040976Z","shell.execute_reply.started":"2025-03-17T19:10:53.00726Z","shell.execute_reply":"2025-03-17T19:10:53.040095Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_new_image_train_phase_2(end_class,max_image_per_class,picked_class=False):\n    main_directory = \"/kaggle/input/processed-imagenet-dataset-192-splitted-train-test/test\"\n    images = []\n    labels = []\n    print(\"Load_new_image_class_test\")\n    if(picked_class):\n        picked_class_folders = ['n01440764', 'n01443537','n01531178','n01537544','n01582220','n01592084','n01601694','n01667778','n01695060','n01698640','n01751748','n01756291','n01770081','n01770393','n01773157','n01774750','n01795545','n01798484','n01806143','n01818515','n01820546','n01824575','n01828970','n01833805','n01843383','n01860187','n01910747','n01914609','n01924916','n01930112','n01950731','n01955084','n01986214','n02006656','n02007558','n02011460','n02013706','n02018207','n02027492','n02037110','n02051845','n02077923']\n        class_folders = picked_class_folders[:end_class]\n    else:\n        class_folders = sorted(os.listdir(main_directory))[:end_class]\n    for label, class_folder in enumerate(class_folders):\n        print(label,class_folder)\n        class_path = os.path.join(main_directory, class_folder)\n        \n        # Check if it's indeed a directory\n        if os.path.isdir(class_path):\n            counter = 0\n            image_names = os.listdir(class_path)\n            random.shuffle(image_names)\n            for image_name in image_names:\n                if(counter>max_image_per_class):\n                    break\n                image_path = os.path.join(class_path, image_name)\n                # Open and resize the image to 150x150\n                img = Image.open(image_path)\n                if(img.mode!='RGB'):\n                    img = np.array(img.convert(\"RGB\"))\n                # Convert image to numpy array and flatten to a 1D array of pixel values\n                img_data = np.array(img)\n                # img_data_with_label = np.concatenate((img_data_with_label, label_data), axis=-1)\n                images.append(img_data)\n                labels.append(label)\n                counter = counter+1\n            print(f\"New Test Image {class_folder} has {counter} non-RGB\")\n    images = np.array(images,dtype=float)\n    images = images/255.0\n    labels = np.array(labels,dtype=int)\n    labels = to_categorical(labels, end_class)\n    return images,labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:10:53.042523Z","iopub.execute_input":"2025-03-17T19:10:53.042875Z","iopub.status.idle":"2025-03-17T19:10:53.054431Z","shell.execute_reply.started":"2025-03-17T19:10:53.042844Z","shell.execute_reply":"2025-03-17T19:10:53.053681Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import libraries\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Dropout\nfrom tensorflow.keras import utils\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import callbacks\nfrom tensorflow.keras import layers, regularizers\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import Callback\nimport time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:10:53.055669Z","iopub.execute_input":"2025-03-17T19:10:53.055921Z","iopub.status.idle":"2025-03-17T19:10:53.489439Z","shell.execute_reply.started":"2025-03-17T19:10:53.055897Z","shell.execute_reply":"2025-03-17T19:10:53.4884Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.applications import ResNet50\n\ndef create_resnet_model(number_of_classes, activation='relu', batch_norm=True, reg='none', dropout=0.2):\n    # Load ResNet50 with pretrained weights on ImageNet, exclude top classification layer\n    base_model = ResNet50(\n        include_top=False,  # Exclude the top layer for custom classification\n        weights='imagenet',  # Use ImageNet-pretrained weights\n        input_shape=(192, 192, 3)  # Adjust input shape as needed\n    )\n    base_model.trainable = True\n    model = tf.keras.Sequential([\n        base_model,\n        layers.GlobalAveragePooling2D(),\n        layers.Dense(512, activation=activation),\n        layers.BatchNormalization(),\n        layers.Dense(number_of_classes, activation='softmax')  # Adjust for multi-class classification\n    ])\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:10:53.490826Z","iopub.execute_input":"2025-03-17T19:10:53.491537Z","iopub.status.idle":"2025-03-17T19:10:53.499315Z","shell.execute_reply.started":"2025-03-17T19:10:53.491487Z","shell.execute_reply":"2025-03-17T19:10:53.49851Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train The Base","metadata":{}},{"cell_type":"code","source":"import time\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nimport matplotlib.pyplot as plt\n\ndef training_base_model(model,class_start,picked_class=False):\n    optimizer = keras.optimizers.Adam(learning_rate=0.00001)\n    model.compile(optimizer=optimizer,\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy']) \n    if(picked_class):\n        picked='picked'\n    else:\n        picked=''\n    MCP = ModelCheckpoint(f'Best_points_{picked}_{class_start}.keras', verbose=1, save_best_only=True, monitor='val_accuracy', mode='max')\n    ES = EarlyStopping(monitor='val_loss', min_delta=0.001, verbose=0, restore_best_weights=True, patience=5, mode='min')\n    \n    start_time = time.time()\n    history = model.fit(train_data,epochs=100,validation_data=test_data,callbacks=[MCP,ES])\n    end_time = time.time()\n    \n    print(\"Training Time: {:.2f} seconds\".format(end_time - start_time))\n    return model,history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:10:53.500788Z","iopub.execute_input":"2025-03-17T19:10:53.501141Z","iopub.status.idle":"2025-03-17T19:10:53.512184Z","shell.execute_reply.started":"2025-03-17T19:10:53.501101Z","shell.execute_reply":"2025-03-17T19:10:53.511259Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_training_result_base(history):\n    # Access the history data\n    history_data = history.history\n    \n    # Plot training & validation accuracy values\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(history_data['accuracy'], label='Training Accuracy')\n    plt.plot(history_data['val_accuracy'], label='Validation Accuracy')\n    plt.title('Model Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend(loc='upper left')\n    \n    # Plot training & validation loss values\n    plt.subplot(1, 2, 2)\n    plt.plot(history_data['loss'], label='Training Loss')\n    plt.plot(history_data['val_loss'], label='Validation Loss')\n    plt.title('Model Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend(loc='upper left')\n    \n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:10:53.513565Z","iopub.execute_input":"2025-03-17T19:10:53.51393Z","iopub.status.idle":"2025-03-17T19:10:53.525449Z","shell.execute_reply.started":"2025-03-17T19:10:53.513891Z","shell.execute_reply":"2025-03-17T19:10:53.524661Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_training_result_extended(training_accuracy, testing_accuracy,training_loss, testing_loss):\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(training_accuracy, label='Training Accuracy')\n    plt.plot(testing_accuracy, label='Validation Accuracy')\n    plt.title('Model Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend(loc='upper left')\n    \n    # Plot training & validation loss values\n    plt.subplot(1, 2, 2)\n    plt.plot(training_loss, label='Training Loss')\n    plt.plot(testing_loss, label='Validation Loss')\n    plt.title('Model Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend(loc='upper left')\n    \n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:10:53.526396Z","iopub.execute_input":"2025-03-17T19:10:53.526684Z","iopub.status.idle":"2025-03-17T19:10:53.537655Z","shell.execute_reply.started":"2025-03-17T19:10:53.526658Z","shell.execute_reply":"2025-03-17T19:10:53.53679Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def confussion_matrix_extended(val_data,model):\n    predictions = model.predict(dataset)\n    predicted_classes = np.argmax(predictions, axis=1)\n    print(\"Classification Report:\")\n    print(classification_report(labels_validation, predicted_classes, target_names=class_folders))\n    macro_f1 = f1_score(labels_validation, predicted_classes, average='macro')\n    print(\"Macro F1-Score:\", macro_f1)\n    conf_matrix = confusion_matrix(labels_validation, predicted_classes)\n    print(\"Confusion Matrix:\")\n    print(conf_matrix)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:10:53.538972Z","iopub.execute_input":"2025-03-17T19:10:53.53971Z","iopub.status.idle":"2025-03-17T19:10:53.545746Z","shell.execute_reply.started":"2025-03-17T19:10:53.539667Z","shell.execute_reply":"2025-03-17T19:10:53.544846Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Now Make The Extended One","metadata":{}},{"cell_type":"markdown","source":"### Make Automate Fully new Model","metadata":{}},{"cell_type":"markdown","source":"## TF.Function","metadata":{}},{"cell_type":"code","source":"@tf.function\ndef distillation_loss(teacher_output, student_output, temperature=2.0):\n    num_teacher_classes = tf.shape(teacher_output)[1]\n    student_output = student_output[:, :num_teacher_classes]\n    teacher_output = tf.nn.softmax(teacher_output / temperature)\n    student_output = tf.nn.softmax(student_output / temperature)\n    return -tf.reduce_sum(teacher_output * tf.math.log(student_output + 1e-8), axis=-1)\n\n@tf.function\ndef combined_loss(y_true, y_pred, teacher_output,alpha=0.5, temperature=2.0):\n    ce_loss = tf.keras.losses.CategoricalCrossentropy()(y_true, y_pred)\n    distill_loss = distillation_loss(teacher_output, y_pred, temperature)\n    return alpha * distill_loss + (1 - alpha) * ce_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:10:53.549614Z","iopub.execute_input":"2025-03-17T19:10:53.549883Z","iopub.status.idle":"2025-03-17T19:10:53.558404Z","shell.execute_reply.started":"2025-03-17T19:10:53.549857Z","shell.execute_reply":"2025-03-17T19:10:53.557504Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Revised Training Code with BIC and corrected Logits","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nclass BiCLayer(tf.keras.layers.Layer):\n    def __init__(self, num_old_classes, num_new_classes):\n        super(BiCLayer, self).__init__()\n        self.num_old_classes = num_old_classes\n        self.num_new_classes = num_new_classes\n        self.alpha = tf.Variable(initial_value=1.0, trainable=True, name=\"alpha\", dtype=tf.float32)\n        self.beta = tf.Variable(initial_value=0.0, trainable=True, name=\"beta\", dtype=tf.float32)\n    \n    def call(self, logits):\n        old_logits = logits[:, :self.num_old_classes]\n        new_logits = logits[:, self.num_old_classes:]\n        corrected_new_logits = self.alpha * new_logits + self.beta\n        corrected_logits = tf.concat([old_logits, corrected_new_logits], axis=1)\n        return corrected_logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:10:53.559647Z","iopub.execute_input":"2025-03-17T19:10:53.560006Z","iopub.status.idle":"2025-03-17T19:10:53.57087Z","shell.execute_reply.started":"2025-03-17T19:10:53.559967Z","shell.execute_reply":"2025-03-17T19:10:53.569986Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Simplified","metadata":{}},{"cell_type":"code","source":"# from sklearn.metrics import classification_report, confusion_matrix,f1_score\n# import numpy as np\n# import gc \n\n# step_class = 5\n# start_class = 5\n# picked_class= True\n\n# rotation_layer = tf.keras.layers.RandomRotation(factor=0.1)\n# def augment(image, label):\n#     image = tf.image.random_flip_left_right(image)\n#     image = tf.image.random_brightness(image, max_delta=0.2)\n#     image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n#     # Use RandomRotation from tf.keras.layers\n#     image = rotation_layer(image)\n#     return image, label\n\n# save_dir = \"./5_step_1_bic\"\n# for number_class_extended in range(15,25,step_class):\n#     #-------------Model-----------------------------\n#     print(number_class_extended,start_class)\n#     if(number_class_extended==start_class):\n#         student_model = create_resnet_model(start_class)\n#     elif(number_class_extended==2*step_class):\n#         try:\n#             del student_model,train_data,test_data,val_data,dataset,dataset_test\n#         except Exception:\n#             pass\n#         model_path = f'{save_dir}/Phase1BIC_best_step_{step_class}_student_model_class_{number_class_extended-2*step_class}.h5'\n#         teacher_model = tf.keras.models.load_model(model_path)\n#         student_model = create_resnet_model(number_class_extended)\n#         num_old_classes, num_new_classes = number_class_extended-(step_class), step_class\n#         teacher_bic_layer = BiCLayer(num_old_classes, num_new_classes)\n#     # # Nyambungin Loss\n#     elif(number_class_extended==15):\n#         model_path = \"/kaggle/working/5_step_1_bic/Phase1BIC_best_step_5_student_model_class_10.h5\"\n#         teacher_model = tf.keras.models.load_model(model_path)\n#         student_model = create_resnet_model(number_class_extended)\n#         #BIC Layer in\n#         num_old_classes = number_class_extended-2*(step_class)\n#         num_new_classes = step_class\n#         teacher_bic_layer = BiCLayer(num_old_classes, num_new_classes)\n#         bic_path = \"/kaggle/working/5_step_1_bic/Phase1BIC_bic_layer_parameters_5_step_10_class.npz\"\n#         # Load the parameters from the .npz file  \n#         data = np.load(bic_path)  \n#         alpha_loaded = data['alpha']  \n#         beta_loaded = data['beta']  \n#         print(f'Loaded Alpha: {alpha_loaded}, Loaded Beta: {beta_loaded}')\n#         teacher_bic_layer.alpha.assign(alpha_loaded)\n#         teacher_bic_layer.beta.assign(beta_loaded)\n#     else:\n#         try:\n#             del teacher_model,train_data,test_data,val_data,dataset,dataset_test,epochs,train_loss,train_accuracy,train_losses,train_accuracies,val_loss,val_losses,val_accuracy,val_accuracies,optimizer,x_batch_val,y_batch_val,x_batch_train,y_batch_train\n#         except Exception:\n#             pass\n#         teacher_model = student_model\n#         del student_model\n#         gc.collect()\n#         student_model = create_resnet_model(number_class_extended)\n#         #BIC Layer in\n#         num_old_classes = number_class_extended-2*(step_class)\n#         num_new_classes = step_class\n#         teacher_bic_layer = BiCLayer(num_old_classes, num_new_classes)\n#         bic_path = f'{save_dir}/Phase1BIC_bic_layer_parameters_{step_class}_step_{number_class_extended-step_class}_class.npz'\n#         # Load the parameters from the .npz file  \n#         data = np.load(bic_path)  \n#         alpha_loaded = data['alpha']  \n#         beta_loaded = data['beta']  \n#         print(f'Loaded Alpha: {alpha_loaded}, Loaded Beta: {beta_loaded}')\n#         teacher_bic_layer.alpha.assign(alpha_loaded)\n#         teacher_bic_layer.beta.assign(beta_loaded)\n#     #------------------Data in-----------------\n#     if(number_class_extended==start_class):\n#         train_data, test_data, val_data = loading_review_ext_train_test_val(number_class_extended-step_class,number_class_extended,picked_class)\n#     else:\n#         train_data, test_data, val_data = loading_buffer_ext_train_test_val(number_class_extended-step_class,number_class_extended,2000,step_class,picked_class)\n#     #Preprocessing\n#     train_data = train_data.map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n#     train_data = train_data.shuffle(buffer_size=1000).batch(16).prefetch(buffer_size=tf.data.AUTOTUNE)\n#     test_data = test_data.batch(16).prefetch(buffer_size=tf.data.AUTOTUNE)\n#     val_data = val_data.batch(16).prefetch(buffer_size=tf.data.AUTOTUNE)\n    \n#     #-------------------Learning------------------------------\n#     if(number_class_extended==start_class):\n#         student_model,history = training_base_model(student_model,number_class_extended,picked_class)\n#         plot_training_result_base(history)\n#         print(f\"Review Accuracy and matix for First {number_class_extended} class\")\n#         student_model.save(f'{save_dir}/Phase1BIC_best_step_{step_class}_student_model_class_{number_class_extended-step_class}.h5')\n#         del history\n#         gc.collect()\n#         continue\n#     else:\n#         print(\"Start Extended Train\")\n#         @tf.function\n#         def training_extended_model_with_bic(student_model, teacher_model, teacher_bic_layer,x_batch_train, y_batch_train, alpha, temperature,optimizer,train_loss,train_accuracy):\n#             with tf.GradientTape() as tape:\n#                 # Forward pass for student model\n#                 student_logits = student_model(x_batch_train, training=True)\n#                 # Forward pass for teacher model (pre-trained model on previous classes)\n#                 teacher_logits = teacher_model(x_batch_train, training=False)\n#                 # tf.print(\"Teacher logits:\", teacher_logits)  # Printing teacher logits\n#                 # # Apply the BiC layer to correct logits\n#                 # teacher_corrected_logits = teacher_bic_layer(teacher_logits)\n#                 # tf.print(2)\n#                 # tf.print(\"Teacher corrected logits:\", teacher_corrected_logits)\n#                 # Compute the combined loss\n#                 total_loss = combined_loss(y_batch_train, student_logits, teacher_logits, alpha, temperature)\n            \n#             # Backpropagation and update student model weights\n#             grads = tape.gradient(total_loss, student_model.trainable_weights)\n#             optimizer.apply_gradients(zip(grads, student_model.trainable_weights))\n            \n#             # Update metrics\n#             train_loss.update_state(total_loss)\n#             train_accuracy.update_state(y_batch_train, student_logits)\n            \n#             return total_loss\n                \n#         @tf.function\n#         def validation_step_with_bic(student_model, teacher_model,teacher_bic_layer, x_batch_val, y_batch_val, temperature):\n#             # Forward pass for student model\n#             student_logits = student_model(x_batch_val, training=False)\n            \n#             # Forward pass for teacher model (pre-trained model on previous classes)\n#             teacher_logits = teacher_model(x_batch_val, training=False)\n#             # Apply the BiC layer to correct logits\n#             teacher_corrected_logits = teacher_bic_layer(teacher_logits)\n#             # Compute the combined loss\n#             total_loss = combined_loss(y_batch_val, student_logits, teacher_corrected_logits, alpha, temperature)\n            \n#             # Update validation metrics\n#             val_loss.update_state(total_loss)\n#             val_accuracy.update_state(y_batch_val, student_logits)\n#         #Loop Parameter\n#         epochs = 100\n#         #Train Loss Parameter\n#         train_loss = tf.keras.metrics.Mean(name='train_loss')\n#         train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n#         val_loss = tf.keras.metrics.Mean(name='val_loss')\n#         val_accuracy = tf.keras.metrics.CategoricalAccuracy(name='val_accuracy')\n#         #Optimizer\n#         optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n#         #Alpha for calculating loss\n#         alpha = round((number_class_extended-step_class)/(number_class_extended),2)\n        \n#         #Temperature for distributing the CE Loss\n#         temperature = 2.0\n#         print(alpha)\n        \n#         #Data in\n#         dataset = train_data\n#         dataset_test = test_data\n        \n#         # Early stopping parameters\n#         patience = 5  # Number of epochs to wait before stopping if no improvement\n#         min_delta = 0.001  # Minimum change in the monitored metric to qualify as an improvement\n#         best_val_loss = np.inf\n#         epochs_without_improvement = 0\n        \n#         #For capturing Loss and Accuracy\n#         # Lists to store metrics for plotting\n#         train_losses = []\n#         train_accuracies = []\n#         val_losses = []\n#         val_accuracies = []\n        \n#         #TRAINING CODE\n#         for epoch in range(epochs):\n#             print(f\"\\nStart of epoch {epoch}\")\n            \n#             # Reset training metrics at the start of each epoch\n#             train_loss.reset_state()\n#             train_accuracy.reset_state()\n        \n#                 # Training loop\n#             for step, (x_batch_train, y_batch_train) in enumerate(dataset):\n#                 loss = training_extended_model_with_bic(student_model, teacher_model, teacher_bic_layer, x_batch_train, y_batch_train, alpha, temperature,optimizer,train_loss,train_accuracy)\n        \n#                 if step % 100 == 0:\n#                     print(f\"Step {step}: Loss = {train_loss.result():.4f}, Accuracy = {train_accuracy.result() * 100:.2f}%\")\n            \n#             # Print training summary for the epoch\n#             print(f\"Epoch {epoch}: Training Loss = {train_loss.result():.4f}, Training Accuracy = {train_accuracy.result() * 100:.2f}%\")\n#             # Store training metrics\n#             train_losses.append(train_loss.result().numpy())\n#             train_accuracies.append(train_accuracy.result().numpy())\n                \n#             # Reset validation metrics before starting validation\n#             val_loss.reset_state()\n#             val_accuracy.reset_state()\n            \n#             # Validation loop\n#             for x_batch_val, y_batch_val in dataset_test:\n#                 validation_step_with_bic(student_model, teacher_model, teacher_bic_layer, x_batch_val, y_batch_val, temperature)\n            \n#             # Print validation summary for the epoch\n#             print(f\"Epoch {epoch}: Validation Loss = {val_loss.result():.4f}, Validation Accuracy = {val_accuracy.result() * 100:.2f}%\")\n                \n#             # Store validation metrics\n#             val_losses.append(val_loss.result().numpy())\n#             val_accuracies.append(val_accuracy.result().numpy())\n#             if epoch == 3:\n#                 new_learning_rate = 0.00001  # Set your new learning rate here\n#                 optimizer.learning_rate.assign(new_learning_rate)\n#                 print(f\"Learning rate updated to: {new_learning_rate}\")\n                \n#             # Early stopping check\n#             current_val_loss = val_loss.result()\n#             if current_val_loss < (best_val_loss - min_delta):\n#                 best_val_loss = current_val_loss\n#                 epochs_without_improvement = 0\n#                 # Optionally, save the model when a new best is found\n#                 student_model.save(f'{save_dir}/Phase1BIC_best_step_{step_class}_student_model_class_{number_class_extended}.h5')\n#             else:\n#                 epochs_without_improvement += 1\n            \n#             if epochs_without_improvement >= patience:\n#                 print(f\"Early stopping triggered after {epoch + 1} epochs due to no improvement.\")\n#                 break\n#         print(f\"Have learned {number_class_extended}\")\n#         plot_training_result_extended(train_accuracies, val_accuracies,train_losses, val_losses)\n#         print()\n\n#     #Validation Set\n#     dataset_val = val_data\n#     main_directory = \"/kaggle/input/processed-imagenet-dataset-192-splitted-train-test/train\"\n#     if(picked_class):\n#         picked_class_folders = ['n01440764', 'n01443537','n01531178','n01537544','n01582220','n01592084','n01601694','n01667778','n01695060','n01698640','n01751748','n01756291','n01770081','n01770393','n01773157','n01774750','n01795545','n01798484','n01806143','n01818515','n01820546','n01824575','n01828970','n01833805','n01843383','n01860187','n01910747','n01914609','n01924916','n01930112','n01950731','n01955084','n01986214','n02006656','n02007558','n02011460','n02013706','n02018207','n02027492','n02037110','n02051845','n02077923']\n#         class_folders= picked_class_folders[:number_class_extended]\n#     else:\n#         class_folders = sorted(os.listdir(main_directory))[:number_class_extended]\n#     print(class_folders)\n#     # Initialize lists to collect predictions and true labels\n#     predictions = []\n#     true_labels = []\n    \n#     # Iterate over the validation dataset\n#     for x_batch_val, y_batch_val in dataset_val:\n#         # Make predictions\n#         preds = student_model.predict(x_batch_val)\n#         predicted_classes = np.argmax(preds, axis=1)\n            \n#         # Collect predictions and true labels\n#         predictions.extend(predicted_classes)\n#         true_labels.extend(np.argmax(y_batch_val.numpy(), axis=1))  # Assuming y_batch_val is one-hot encoded\n        \n#     # Convert lists to numpy arrays\n#     predictions = np.array(predictions)\n#     true_labels = np.array(true_labels)\n        \n#     # Calculate metrics\n#     print(\"Classification Report:\")\n#     print(classification_report(true_labels, predictions, target_names=class_folders))\n        \n#     # Calculate macro F1-score directly\n#     macro_f1 = f1_score(true_labels, predictions, average='macro')\n#     print(\"Macro F1-Score:\", macro_f1)\n        \n#         # Confusion matrix (optional, for deeper analysis)\n#     conf_matrix = confusion_matrix(true_labels, predictions)\n#     print(\"Confusion Matrix:\")\n#     print(conf_matrix)\n\n#     #Training Phase II\n#     #Clear all data from Phase 1\n#     del train_data, test_data, val_data, student_model\n#     try:\n#         del teacher_model,dataset,dataset_test,epochs,train_loss,train_accuracy,train_losses,train_accuracies,val_loss,val_losses,val_accuracy,val_accuracies,optimizer,x_batch_val,y_batch_val,x_batch_train,y_batch_train\n#     except Exception:\n#         pass\n#     gc.collect()\n    \n#     # Step 3: Create the BiC layer model\n#     num_old_classes = number_class_extended - step_class  # Adjust according to your setup\n#     num_new_classes = step_class  # Adjust according to your setup\n#     number_sample = round(2000/(10*num_old_classes))*3\n#     bic_layer = BiCLayer(num_old_classes, num_new_classes)\n#     if(picked_class):\n#         picked_class_folders = ['n01440764', 'n01443537','n01531178','n01537544','n01582220','n01592084','n01601694','n01667778','n01695060','n01698640','n01751748','n01756291','n01770081','n01770393','n01773157','n01774750','n01795545','n01798484','n01806143','n01818515','n01820546','n01824575','n01828970','n01833805','n01843383','n01860187','n01910747','n01914609','n01924916','n01930112','n01950731','n01955084','n01986214','n02006656','n02007558','n02011460','n02013706','n02018207','n02027492','n02037110','n02051845','n02077923']\n#         class_folders= picked_class_folders[:num_old_classes+num_new_classes]\n#     else:\n#         class_folders = sorted(os.listdir(main_directory))[:num_old_classes+num_new_classes]\n    \n#     #model in\n#     model_path = f'{save_dir}/Phase1BIC_best_step_{step_class}_student_model_class_{number_class_extended}.h5'\n#     student_model = tf.keras.models.load_model(model_path)\n#     student_model.summary()\n    \n#     def loading_bic_test_val(end_class,number_images,picked_class=False):\n#         images_test, labels_test = load_new_image_train_phase_2(end_class,number_images,picked_class)\n#         test_dataset = tf.data.Dataset.from_tensor_slices((images_test, labels_test))\n#         del images_test, labels_test\n#         images_val, labels_val = load_new_image_class_validation(end_class,picked_class)\n#         validation_dataset = tf.data.Dataset.from_tensor_slices((images_val, labels_val))\n#         del images_val, labels_val\n#         return test_dataset, validation_dataset\n    \n#     test_data, val_data = loading_bic_test_val(num_new_classes+num_old_classes,number_sample, picked_class)\n#     test_data = test_data.batch(32).prefetch(buffer_size=tf.data.AUTOTUNE)\n#     dataset_test = test_data\n#     dataset_val = val_data.batch(16)\n    \n#     import numpy as np\n#     from sklearn.metrics import classification_report, f1_score, confusion_matrix\n#     import tensorflow as tf\n#     print(\"/////////////////// NON BIC //////////////////\")\n#     # Initialize lists to collect predictions and true labels\n#     predictions_non_bic = []\n#     true_labels_non_bic = []\n        \n#     # Iterate over the validation dataset\n#     for x_batch_val, y_batch_val in dataset_val:\n#         # Make predictions\n#         preds = student_model.predict(x_batch_val)\n#         predicted_classes_non_bic = np.argmax(preds, axis=1)\n                \n#         # Collect predictions and true labels\n#         predictions_non_bic.extend(predicted_classes_non_bic)\n#         true_labels_non_bic.extend(np.argmax(y_batch_val.numpy(), axis=1))  # Assuming y_batch_val is one-hot encoded\n            \n#         # Convert lists to numpy arrays\n#     predictions = np.array(predictions_non_bic)\n#     true_labels = np.array(true_labels_non_bic)\n            \n#     # Calculate metrics\n#     print(\"Classification Report:\")\n#     print(classification_report(true_labels, predictions, target_names=class_folders))\n            \n#     # Calculate macro F1-score directly\n#     macro_f1 = f1_score(true_labels, predictions, average='macro')\n#     print(\"Macro F1-Score:\", macro_f1)\n            \n#     # Confusion matrix (optional, for deeper analysis)\n#     conf_matrix = confusion_matrix(true_labels, predictions)\n#     print(\"Confusion Matrix:\")\n#     print(conf_matrix)\n    \n#     print(bic_layer.alpha.numpy(),bic_layer.beta.numpy())\n    \n#     from collections import defaultdict\n    \n#     # Create a dictionary to store data and labels for each class\n#     data_by_class = defaultdict(list)\n    \n#     # Unbatch the dataset\n#     for data, label in dataset_test.unbatch():  # Replace `32` with your batch size if known\n#         label_value = tf.argmax(label).numpy() if len(label.shape) > 0 and label.shape[-1] > 1 else label.numpy()\n#         data_by_class[label_value].append((data.numpy(), label_value))\n    \n#     # Print the number of samples for each class\n#     for class_label, samples in data_by_class.items():\n#         print(f\"Class {class_label} has {len(samples)} samples.\")\n    \n#     # Combine the data and labels from all classes into a single list\n#     combined_data = []\n#     for class_label, samples in data_by_class.items():\n#         combined_data.extend(samples)  # Each `samples` is a list of (data, label) tuples\n    \n#     # Shuffle the combined data\n#     np.random.shuffle(combined_data)\n    \n#     # Separate features and labels\n#     features = np.array([sample[0] for sample in combined_data])\n#     labels = np.array([sample[1] for sample in combined_data])\n    \n#     # Create a tf.data.Dataset and batch it with a batch size of 32\n#     batched_dataset = tf.data.Dataset.from_tensor_slices((features, labels)).batch(32)\n    \n#     # Print the number of batches\n#     print(f\"The dataset has been batched into {len(list(batched_dataset))} batches.\")\n    \n    \n#     # Early stopping parameters\n#     patience = 5  # Number of epochs to wait before stopping if no improvement\n#     min_delta = 0.001  # Minimum change in the monitored metric to qualify as an improvement\n#     best_val_loss = np.inf\n#     epochs_without_improvement = 0\n#     train_loss_bic = tf.keras.metrics.Mean(name='train_loss')\n#     train_accuracy_bic = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n#     bic_optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n#     epochs = 100\n    \n#     @tf.function\n#     def training_bic_model(student_model,bic_layer, x_batch_train, y_batch_train):\n#         with tf.GradientTape() as tape:\n#             # Forward pass for student model\n#             logits = student_model(x_batch_train, training=False)\n#             logits = tf.stop_gradient(logits)\n#             corrected_logits = bic_layer(logits)  # Apply BiC layer\n            \n#             # # Use tf.print to print tensor values  \n#             # tf.print(\"Logits:\", logits)\n#             # tf.print(\"Corrected_logits\", corrected_logits)\n#             loss = tf.reduce_mean(\n#                 tf.keras.losses.categorical_crossentropy(y_batch_train, corrected_logits, from_logits=True)\n#             )\n        \n#         # Update gradients only for the BiC layer parameters\n#         gradients = tape.gradient(loss, [bic_layer.alpha, bic_layer.beta])\n#         bic_optimizer.apply_gradients(zip(gradients, [bic_layer.alpha, bic_layer.beta]))\n    \n#         # Update training metrics\n#         train_loss_bic.update_state(loss)\n#         train_accuracy_bic.update_state(y_batch_train, tf.nn.softmax(corrected_logits))\n#         return loss\n    \n#     # Step 4: Apply BiC layer on logits generated by the student model\n#     for epoch in range(epochs):\n#         print(f\"\\nStart of epoch {epoch}\")\n    \n#         # Reset training metrics\n#         train_loss_bic.reset_state()\n#         train_accuracy_bic.reset_state()\n    \n#         for step, (x_batch_train, y_batch_train) in enumerate(batched_dataset):\n#             y_batch_train = tf.one_hot(y_batch_train, depth=num_old_classes+num_new_classes)\n#             loss_bic = training_bic_model(student_model,bic_layer,x_batch_train, y_batch_train)\n    \n#             if step % 100 == 0:\n#                 print(f\"Step {step}: Loss = {train_loss_bic.result():.4f}, Accuracy = {train_accuracy_bic.result() * 100:.2f}%\")\n#         print(bic_layer.alpha.numpy(),bic_layer.beta.numpy())\n#         # Print training summary for the epoch\n#         print(f\"Epoch {epoch}: Training Loss = {train_loss_bic.result():.4f}, Training Accuracy = {train_accuracy_bic.result() * 100:.2f}%\")\n    \n#         # Validation and early stopping code (similar to previous implementation)\n#         current_val_loss = train_loss_bic.result()\n#         if current_val_loss < (best_val_loss - min_delta):\n#             best_val_loss = current_val_loss\n#             epochs_without_improvement = 0\n#             # Optionally, save the model when a new best is found\n#             student_model.save(f'{save_dir}/Phase1BIC_best_bic_student_model_epoch_custom_{num_old_classes+num_new_classes}_class.h5')\n#         else:\n#             epochs_without_improvement += 1\n    \n#         if epochs_without_improvement >= patience:\n#             print(f\"Early stopping triggered after {epoch + 1} epochs due to no improvement.\")\n#             break\n#     student_model.save(f'{save_dir}/Phase1BIC_best_bic_student_model_epoch_custom_{num_old_classes+num_new_classes}_class.h5')\n#     # Step 5: Save the Numpy Alpha Beta BIC Parameter\n#     # Assuming bic_layer is an instance of BiCLayer  \n#     alpha_value = bic_layer.alpha.numpy()  \n#     beta_value = bic_layer.beta.numpy()  \n      \n#     # Save alpha and beta to a .npz file  \n#     np.savez(f'{save_dir}/Phase1BIC_bic_layer_parameters_{step_class}_step_{num_old_classes+num_new_classes}_class.npz', alpha=alpha_value, beta=beta_value)  \n#     # Step 6 : Validate with BiC layer\n#     def predict_with_bic(student_model, bic_layer, dataset):\n#         all_predictions = []\n#         all_true_labels = []\n        \n#         for x_batch, y_batch in dataset:\n#             # Get logits from the student model\n#             logits = student_model(x_batch, training=False)\n            \n#             # Apply the BiC layer to correct logits\n#             corrected_logits = bic_layer(logits)\n#             # Apply softmax to get probabilities\n#             probabilities = tf.nn.softmax(corrected_logits)\n#             # Get predicted classes\n#             predicted_classes = tf.argmax(probabilities, axis=-1).numpy()\n            \n#             # Collect predictions and true labels\n#             all_predictions.extend(predicted_classes)\n#             all_true_labels.extend(y_batch.numpy())  # Ensure y_batch is in the correct format\n        \n#         return np.array(all_predictions), np.array(all_true_labels)\n    \n#     # Predict with the model and BiC layer\n#     predicted_classes, true_labels = predict_with_bic(student_model, bic_layer, dataset_val)\n    \n#     # Convert true labels to class indices if they are one-hot encoded\n#     if true_labels.ndim > 1:  # Check if true_labels is one-hot encoded\n#         true_labels = np.argmax(true_labels, axis=1)\n    \n#     if(picked_class):\n#         picked_class_folders = ['n01440764', 'n01443537','n01531178','n01537544','n01582220','n01592084','n01601694','n01667778','n01695060','n01698640','n01751748','n01756291','n01770081','n01770393','n01773157','n01774750','n01795545','n01798484','n01806143','n01818515','n01820546','n01824575','n01828970','n01833805','n01843383','n01860187','n01910747','n01914609','n01924916','n01930112','n01950731','n01955084','n01986214','n02006656','n02007558','n02011460','n02013706','n02018207','n02027492','n02037110','n02051845','n02077923']\n#         class_folders= picked_class_folders[:num_old_classes+num_new_classes]\n#     else:\n#         class_folders = sorted(os.listdir(main_directory))[:num_old_classes+num_new_classes]\n    \n#     # Calculate metrics\n#     print(\"Classification Report:\")\n#     print(classification_report(true_labels, predicted_classes, target_names=class_folders))\n    \n#     # Calculate macro F1-score directly\n#     macro_f1 = f1_score(true_labels, predicted_classes, average='macro')\n#     print(\"Macro F1-Score:\", macro_f1)\n    \n#     # Confusion matrix (optional, for deeper analysis)\n#     conf_matrix = confusion_matrix(true_labels, predicted_classes)\n#     print(\"Confusion Matrix:\")\n#     print(conf_matrix)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:10:53.572276Z","iopub.execute_input":"2025-03-17T19:10:53.572623Z","iopub.status.idle":"2025-03-17T19:10:53.591891Z","shell.execute_reply.started":"2025-03-17T19:10:53.572594Z","shell.execute_reply":"2025-03-17T19:10:53.590986Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training Without Any BIC","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix,f1_score, accuracy_score\nimport numpy as np\nimport gc \n\nstep_class = 5\nstart_class = 5\npicked_class= True\n\nrotation_layer = tf.keras.layers.RandomRotation(factor=0.1)\ndef augment(image, label):\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_brightness(image, max_delta=0.2)\n    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n    # Use RandomRotation from tf.keras.layers\n    image = rotation_layer(image)\n    return image, label\n\nsave_dir = \"./5_step_no_bic/\"\nfor number_class_extended in range(20,25,step_class):\n    #-------------Model-----------------------------\n    print(number_class_extended,start_class)\n    if(number_class_extended==start_class):\n        student_model = create_resnet_model(start_class)\n    elif(number_class_extended==2*step_class):\n        try:\n            del student_model,train_data,test_data,val_data,dataset,dataset_test\n        except Exception:\n            pass\n            \n        model_path = f'{save_dir}/NOBIC_best_NOBIC_student_model_class_{number_class_extended-step_class}.h5'\n        teacher_model = tf.keras.models.load_model(model_path)\n        student_model = create_resnet_model(number_class_extended)\n    # Nyambungin Loss\n    elif(number_class_extended==20):\n        model_path = \"/kaggle/working/5_step_no_bic/NOBIC_5_step_best_NOBIC_student_model_class_15.h5\"\n        teacher_model = tf.keras.models.load_model(model_path)\n        student_model = create_resnet_model(number_class_extended)\n    else:\n        try:\n            del teacher_model,train_data,test_data,val_data,dataset,dataset_test,epochs,train_loss,train_accuracy,train_losses,train_accuracies,val_loss,val_losses,val_accuracy,val_accuracies,optimizer,x_batch_val,y_batch_val,x_batch_train,y_batch_train\n        except Exception:\n            pass\n        teacher_model = student_model\n        del student_model\n        gc.collect()\n        student_model = create_resnet_model(number_class_extended)\n    #------------------Data in-----------------\n    if(number_class_extended==start_class):\n        train_data, test_data, val_data = loading_review_ext_train_test_val(number_class_extended-step_class,number_class_extended,picked_class)\n    else:\n        train_data, test_data, val_data = loading_buffer_ext_train_test_val(number_class_extended-step_class,number_class_extended,2000,step_class,picked_class)\n    #Preprocessing\n    train_data = train_data.map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n    train_data = train_data.shuffle(buffer_size=1000).batch(16).prefetch(buffer_size=tf.data.AUTOTUNE)\n    test_data = test_data.batch(16).prefetch(buffer_size=tf.data.AUTOTUNE)\n    val_data = val_data.batch(16).prefetch(buffer_size=tf.data.AUTOTUNE)\n    \n    #-------------------Learning------------------------------\n    if(number_class_extended==start_class):\n        student_model,history = training_base_model(student_model,number_class_extended,picked_class)\n        plot_training_result_base(history)\n        print(f\"Review Accuracy and matix for First {number_class_extended} class\")\n        student_model.save(f'{save_dir}/NOBIC_best_NOBIC_student_model_class_{number_class_extended}.h5')\n        del history\n        gc.collect()\n        continue\n    else:\n        print(\"Start Extended Train\")\n        @tf.function\n        def training_extended_model_no_bic(student_model, teacher_model,x_batch_train, y_batch_train, alpha, temperature,optimizer,train_loss,train_accuracy):\n            with tf.GradientTape() as tape:\n                # Forward pass for student model\n                student_logits = student_model(x_batch_train, training=True)\n                # Forward pass for teacher model (pre-trained model on previous classes)\n                teacher_logits = teacher_model(x_batch_train, training=False)\n                # Compute the combined loss\n                total_loss = combined_loss(y_batch_train, student_logits, teacher_logits, alpha, temperature)\n            \n            # Backpropagation and update student model weights\n            grads = tape.gradient(total_loss, student_model.trainable_weights)\n            optimizer.apply_gradients(zip(grads, student_model.trainable_weights))\n            \n            # Update metrics\n            train_loss.update_state(total_loss)\n            train_accuracy.update_state(y_batch_train, student_logits)\n            \n            return total_loss\n                \n        @tf.function\n        def validation_step_no_bic(student_model, teacher_model, x_batch_val, y_batch_val, temperature):\n            # Forward pass for student model\n            student_logits = student_model(x_batch_val, training=False)\n            \n            # Forward pass for teacher model (pre-trained model on previous classes)\n            teacher_logits = teacher_model(x_batch_val, training=False)\n            # Compute the combined loss\n            total_loss = combined_loss(y_batch_val, student_logits, teacher_logits, alpha, temperature)\n            # Update validation metrics\n            val_loss.update_state(total_loss)\n            val_accuracy.update_state(y_batch_val, student_logits)\n        #Loop Parameter\n        epochs = 100\n        #Train Loss Parameter\n        train_loss = tf.keras.metrics.Mean(name='train_loss')\n        train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n        val_loss = tf.keras.metrics.Mean(name='val_loss')\n        val_accuracy = tf.keras.metrics.CategoricalAccuracy(name='val_accuracy')\n        #Optimizer\n        optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n        #Alpha for calculating loss\n        alpha = round((number_class_extended-step_class)/(number_class_extended),2)\n        \n        #Temperature for distributing the CE Loss\n        temperature = 2.0\n        print(alpha)\n        \n        #Data in\n        dataset = train_data\n        dataset_test = test_data\n        \n        # Early stopping parameters\n        patience = 5  # Number of epochs to wait before stopping if no improvement\n        min_delta = 0.001  # Minimum change in the monitored metric to qualify as an improvement\n        best_val_loss = np.inf\n        epochs_without_improvement = 0\n        \n        #For capturing Loss and Accuracy\n        # Lists to store metrics for plotting\n        train_losses = []\n        train_accuracies = []\n        val_losses = []\n        val_accuracies = []\n        \n        #TRAINING CODE\n        for epoch in range(epochs):\n            print(f\"\\nStart of epoch {epoch}\")\n            \n            # Reset training metrics at the start of each epoch\n            train_loss.reset_state()\n            train_accuracy.reset_state()\n        \n                # Training loop\n            for step, (x_batch_train, y_batch_train) in enumerate(dataset):\n                loss = training_extended_model_no_bic(student_model, teacher_model, x_batch_train, y_batch_train, alpha, temperature,optimizer,train_loss,train_accuracy)\n            \n            # Print training summary for the epoch\n            print(f\"Epoch {epoch}: Training Loss = {train_loss.result():.4f}, Training Accuracy = {train_accuracy.result() * 100:.2f}%\")\n            # Store training metrics\n            train_losses.append(train_loss.result().numpy())\n            train_accuracies.append(train_accuracy.result().numpy())\n                \n            # Reset validation metrics before starting validation\n            val_loss.reset_state()\n            val_accuracy.reset_state()\n            \n            # Validation loop\n            for x_batch_val, y_batch_val in dataset_test:\n                validation_step_no_bic(student_model, teacher_model, x_batch_val, y_batch_val, temperature)\n            \n            # Print validation summary for the epoch\n            print(f\"Epoch {epoch}: Validation Loss = {val_loss.result():.4f}, Validation Accuracy = {val_accuracy.result() * 100:.2f}%\")\n                \n            # Store validation metrics\n            val_losses.append(val_loss.result().numpy())\n            val_accuracies.append(val_accuracy.result().numpy())\n            if epoch == 3:\n                new_learning_rate = 0.00001  # Set your new learning rate here\n                optimizer.learning_rate.assign(new_learning_rate)\n                print(f\"Learning rate updated to: {new_learning_rate}\")\n                \n            # Early stopping check\n            current_val_loss = val_loss.result()\n            if current_val_loss < (best_val_loss - min_delta):\n                best_val_loss = current_val_loss\n                epochs_without_improvement = 0\n                # Optionally, save the model when a new best is found\n                student_model.save(f'{save_dir}/NOBIC_{step_class}_step_best_NOBIC_student_model_class_{number_class_extended}.h5')\n            else:\n                epochs_without_improvement += 1\n            \n            if epochs_without_improvement >= patience:\n                print(f\"Early stopping triggered after {epoch + 1} epochs due to no improvement.\")\n                student_model.save(f'{save_dir}/NOBIC_{step_class}_step_best_NOBIC_loss_student_model_class_{number_class_extended}.h5')\n                break\n        print(f\"Have learned {number_class_extended}\")\n        plot_training_result_extended(train_accuracies, val_accuracies,train_losses, val_losses)\n        print()\n\n    #Validation Set\n    dataset_val = val_data\n    main_directory = \"/kaggle/input/processed-imagenet-dataset-192-splitted-train-test/train\"\n    if(picked_class):\n        picked_class_folders = ['n01440764', 'n01443537','n01531178','n01537544','n01582220','n01592084','n01601694','n01667778','n01695060','n01698640','n01751748','n01756291','n01770081','n01770393','n01773157','n01774750','n01795545','n01798484','n01806143','n01818515','n01820546','n01824575','n01828970','n01833805','n01843383','n01860187','n01910747','n01914609','n01924916','n01930112','n01950731','n01955084','n01986214','n02006656','n02007558','n02011460','n02013706','n02018207','n02027492','n02037110','n02051845','n02077923']\n        class_folders= picked_class_folders[:number_class_extended]\n    else:\n        class_folders = sorted(os.listdir(main_directory))[:number_class_extended]\n    print(class_folders)\n    # Initialize lists to collect predictions and true labels\n    predictions = []\n    true_labels = []\n    \n    # Iterate over the validation dataset\n    for x_batch_val, y_batch_val in dataset_val:\n        # Make predictions\n        preds = student_model.predict(x_batch_val)\n        predicted_classes = np.argmax(preds, axis=1)\n            \n        # Collect predictions and true labels\n        predictions.extend(predicted_classes)\n        true_labels.extend(np.argmax(y_batch_val.numpy(), axis=1))  # Assuming y_batch_val is one-hot encoded\n        \n    # Convert lists to numpy arrays\n    predictions = np.array(predictions)\n    true_labels = np.array(true_labels)\n        \n    # Calculate metrics\n    print(\"Classification Report:\")\n    print(classification_report(true_labels, predictions, target_names=class_folders))\n        \n    # Calculate macro F1-score directly\n    macro_f1 = f1_score(true_labels, predictions, average='macro')\n    print(\"Macro F1-Score:\", macro_f1)\n\n    macro_accuracy = accuracy_score(true_labels, predictions)\n    print(\"Macro Accuracy:\", macro_accuracy)\n        \n    # Confusion matrix (optional, for deeper analysis)\n    conf_matrix = confusion_matrix(true_labels, predictions)\n    print(\"Confusion Matrix:\")\n    print(conf_matrix)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:10:53.593332Z","iopub.execute_input":"2025-03-17T19:10:53.593793Z","iopub.status.idle":"2025-03-17T19:56:49.292928Z","shell.execute_reply.started":"2025-03-17T19:10:53.593763Z","shell.execute_reply":"2025-03-17T19:56:49.291923Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.metrics import classification_report, confusion_matrix,f1_score, accuracy_score\n# #Validation Set\n# dataset_val = val_data\n# main_directory = \"/kaggle/input/processed-imagenet-dataset-192-splitted-train-test/train\"\n# if(picked_class):\n#     picked_class_folders = ['n01440764', 'n01443537','n01531178','n01537544','n01582220','n01592084','n01601694','n01667778','n01695060','n01698640','n01751748','n01756291','n01770081','n01770393','n01773157','n01774750','n01795545','n01798484','n01806143','n01818515','n01820546','n01824575','n01828970','n01833805','n01843383','n01860187','n01910747','n01914609','n01924916','n01930112','n01950731','n01955084','n01986214','n02006656','n02007558','n02011460','n02013706','n02018207','n02027492','n02037110','n02051845','n02077923']\n#     class_folders= picked_class_folders[:number_class_extended]\n# else:\n#     class_folders = sorted(os.listdir(main_directory))[:number_class_extended]\n# print(class_folders)\n# # Initialize lists to collect predictions and true labels\n# predictions = []\n# true_labels = []\n\n# # Iterate over the validation dataset\n# for x_batch_val, y_batch_val in dataset_val:\n#     # Make predictions\n#     preds = student_model.predict(x_batch_val)\n#     predicted_classes = np.argmax(preds, axis=1)\n        \n#     # Collect predictions and true labels\n#     predictions.extend(predicted_classes)\n#     true_labels.extend(np.argmax(y_batch_val.numpy(), axis=1))  # Assuming y_batch_val is one-hot encoded\n    \n# # Convert lists to numpy arrays\n# predictions = np.array(predictions)\n# true_labels = np.array(true_labels)\n    \n# # Calculate metrics\n# print(\"Classification Report:\")\n# print(classification_report(true_labels, predictions, target_names=class_folders))\n    \n# # Calculate macro F1-score directly\n# macro_f1 = f1_score(true_labels, predictions, average='macro')\n# print(\"Macro F1-Score:\", macro_f1)\n# macro_accuracy = accuracy_score(true_labels, predictions)\n# print(\"Macro Accuracy:\", macro_accuracy)\n    \n# # Confusion matrix (optional, for deeper analysis)\n# conf_matrix = confusion_matrix(true_labels, predictions)\n# print(\"Confusion Matrix:\")\n# print(conf_matrix)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-17T11:24:27.009Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Custom BIC","metadata":{}},{"cell_type":"code","source":"# picked_class=True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T07:07:03.806479Z","iopub.execute_input":"2025-02-17T07:07:03.807102Z","iopub.status.idle":"2025-02-17T07:07:03.810533Z","shell.execute_reply.started":"2025-02-17T07:07:03.807067Z","shell.execute_reply":"2025-02-17T07:07:03.809705Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Step 3: Create the BiC layer model\n# num_old_classes = 5  # Adjust according to your setup\n# num_new_classes = 5  # Adjust according to your setup\n# bic_layer = BiCLayer(num_old_classes, num_new_classes)\n# if(picked_class):\n#     picked_class_folders = ['n01440764', 'n01443537','n01531178','n01537544','n01582220','n01592084','n01601694','n01667778','n01695060','n01698640','n01751748','n01756291','n01770081','n01770393','n01773157','n01774750','n01795545','n01798484','n01806143','n01818515','n01820546','n01824575','n01828970','n01833805','n01843383','n01860187','n01910747','n01914609','n01924916','n01930112','n01950731','n01955084','n01986214','n02006656','n02007558','n02011460','n02013706','n02018207','n02027492','n02037110','n02051845','n02077923']\n#     class_folders= picked_class_folders[:num_old_classes+num_new_classes]\n# else:\n#     class_folders = sorted(os.listdir(main_directory))[:num_old_classes+num_new_classes]\n\n# #model in\n# model_path = '/kaggle/input/student-model-10-class/tensorflow2/default/1/best_student_model_class_10 (2).h5'\n# student_model = tf.keras.models.load_model(model_path)\n# student_model.summary()\n\n# # # Freeze all layers in the model\n# # for layer in student_model.layers:\n# #     layer.trainable = True # Ensure each layer is frozen\n\n# # Confirm\n# for layer in student_model.layers:\n#     print(layer.name, layer.trainable)  # Should output False for all layers\n\n\n# def loading_bic_test_val(end_class,picked_class=False):\n#     images_test, labels_test = load_new_image_class_test(end_class,picked_class)\n#     test_dataset = tf.data.Dataset.from_tensor_slices((images_test, labels_test))\n#     del images_test, labels_test\n#     images_val, labels_val = load_new_image_class_validation(end_class,picked_class)\n#     validation_dataset = tf.data.Dataset.from_tensor_slices((images_val, labels_val))\n#     del images_val, labels_val\n#     return test_dataset, validation_dataset\n\n# test_data, val_data = loading_bic_test_val(num_new_classes+num_old_classes,picked_class)\n# test_data = test_data.batch(32).prefetch(buffer_size=tf.data.AUTOTUNE)\n# dataset_test = test_data\n\n# dataset_val = val_data.batch(16)\n# import numpy as np\n# from sklearn.metrics import classification_report, f1_score, confusion_matrix\n# import tensorflow as tf\n# print(\"/////////////////// NON BIC //////////////////\")\n# # Initialize lists to collect predictions and true labels\n# predictions_non_bic = []\n# true_labels_non_bic = []\n    \n# # Iterate over the validation dataset\n# for x_batch_val, y_batch_val in dataset_val:\n#     # Make predictions\n#     preds = student_model.predict(x_batch_val)\n#     predicted_classes_non_bic = np.argmax(preds, axis=1)\n            \n#     # Collect predictions and true labels\n#     predictions_non_bic.extend(predicted_classes_non_bic)\n#     true_labels_non_bic.extend(np.argmax(y_batch_val.numpy(), axis=1))  # Assuming y_batch_val is one-hot encoded\n        \n#     # Convert lists to numpy arrays\n# predictions = np.array(predictions_non_bic)\n# true_labels = np.array(true_labels_non_bic)\n        \n# # Calculate metrics\n# print(\"Classification Report:\")\n# print(classification_report(true_labels, predictions, target_names=class_folders))\n        \n# # Calculate macro F1-score directly\n# macro_f1 = f1_score(true_labels, predictions, average='macro')\n# print(\"Macro F1-Score:\", macro_f1)\n        \n# # Confusion matrix (optional, for deeper analysis)\n# conf_matrix = confusion_matrix(true_labels, predictions)\n# print(\"Confusion Matrix:\")\n# print(conf_matrix)\n\n# import tensorflow as tf\n# from collections import defaultdict\n\n# # Create a dictionary to store data and labels for each class\n# data_by_class = defaultdict(list)\n\n# import tensorflow as tf\n# from collections import defaultdict\n\n# # Create a dictionary to store data and labels for each class\n# data_by_class = defaultdict(list)\n\n# # Unbatch the dataset\n# for data, label in dataset_test.unbatch():  # Replace `32` with your batch size if known\n#     label_value = tf.argmax(label).numpy() if len(label.shape) > 0 and label.shape[-1] > 1 else label.numpy()\n#     data_by_class[label_value].append((data.numpy(), label_value))\n\n# # Print the number of samples for each class\n# for class_label, samples in data_by_class.items():\n#     print(f\"Class {class_label} has {len(samples)} samples.\")\n\n# # Combine the data and labels from all classes into a single list\n# combined_data = []\n# for class_label, samples in data_by_class.items():\n#     combined_data.extend(samples)  # Each `samples` is a list of (data, label) tuples\n\n# # Shuffle the combined data\n# np.random.shuffle(combined_data)\n\n# # Separate features and labels\n# features = np.array([sample[0] for sample in combined_data])\n# labels = np.array([sample[1] for sample in combined_data])\n\n# # Create a tf.data.Dataset and batch it with a batch size of 32\n# batched_dataset = tf.data.Dataset.from_tensor_slices((features, labels)).batch(32)\n\n# # Print the number of batches\n# print(f\"The dataset has been batched into {len(list(batched_dataset))} batches.\")\n\n\n# # Early stopping parameters\n# patience = 5  # Number of epochs to wait before stopping if no improvement\n# min_delta = 0.001  # Minimum change in the monitored metric to qualify as an improvement\n# best_val_loss = np.inf\n# epochs_without_improvement = 0\n# train_loss_bic = tf.keras.metrics.Mean(name='train_loss')\n# train_accuracy_bic = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n# student_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n# bic_optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n# epochs = 20\n# save_dir = \"./\"\n# @tf.function\n# def training_bic_model(student_model,bic_layer, x_batch_train, y_batch_train):\n#     with tf.GradientTape() as tape:\n#         # Forward pass for student model\n#         logits = student_model(x_batch_train, training=True)\n#         logits = tf.stop_gradient(logits)\n#         corrected_logits = bic_layer(logits)  # Apply BiC layer\n        \n#         # # Use tf.print to print tensor values  \n#         # tf.print(\"Logits:\", logits)  \n#         loss = tf.keras.losses.categorical_crossentropy(y_batch_train, corrected_logits, from_logits=True)\n    \n#     # Update gradients only for the BiC layer parameters\n#     gradients = tape.gradient(loss, [bic_layer.alpha, bic_layer.beta])\n#     bic_optimizer.apply_gradients(zip(gradients, [bic_layer.alpha, bic_layer.beta]))\n\n#     # Update training metrics\n#     train_loss_bic.update_state(loss)\n#     train_accuracy_bic.update_state(y_batch_train, tf.nn.softmax(corrected_logits))\n\n#     return loss\n\n# # Step 4: Apply BiC layer on logits generated by the student model\n# for epoch in range(epochs):\n#     print(f\"\\nStart of epoch {epoch}\")\n\n#     # Reset training metrics\n#     train_loss_bic.reset_state()\n#     train_accuracy_bic.reset_state()\n\n#     for step, (x_batch_train, y_batch_train) in enumerate(batched_dataset):\n#         y_batch_train = tf.one_hot(y_batch_train, depth=num_old_classes+num_new_classes)\n#         loss_bic = training_bic_model(student_model,bic_layer,x_batch_train, y_batch_train)\n\n#         if step % 100 == 0:\n#             print(f\"Step {step}: Loss = {train_loss_bic.result():.4f}, Accuracy = {train_accuracy_bic.result() * 100:.2f}%\")\n\n#     # Print training summary for the epoch\n#     print(f\"Epoch {epoch}: Training Loss = {train_loss_bic.result():.4f}, Training Accuracy = {train_accuracy_bic.result() * 100:.2f}%\")\n\n#     # Validation and early stopping code (similar to previous implementation)\n#     current_val_loss = train_loss_bic.result()\n#     if current_val_loss < (best_val_loss - min_delta):\n#         best_val_loss = current_val_loss\n#         epochs_without_improvement = 0\n#         # Optionally, save the model when a new best is found\n#         student_model.save(f'{save_dir}/best_bic_student_model_epoch_custom_{num_old_classes+num_new_classes}_class.h5')\n#     else:\n#         epochs_without_improvement += 1\n\n#     if epochs_without_improvement >= patience:\n#         print(f\"Early stopping triggered after {epoch + 1} epochs due to no improvement.\")\n#         break\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T07:07:03.915401Z","iopub.execute_input":"2025-02-17T07:07:03.915685Z","iopub.status.idle":"2025-02-17T07:07:03.923541Z","shell.execute_reply.started":"2025-02-17T07:07:03.915659Z","shell.execute_reply":"2025-02-17T07:07:03.922707Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Review Model!","metadata":{}},{"cell_type":"code","source":"# Function to get predictions using the BiC layer\nstudent_scenario_1 = tf.keras.models.load_model(\"/kaggle/input/evaluate-model-b/tensorflow2/default/1/NOBIC/2_step_best_NOBIC_student_model_class_20.h5\")\nstudent_scenario_2 = tf.keras.models.load_model(\"/kaggle/input/evaluate-model-b/tensorflow2/default/1/NOBIC/4_step_best_NOBIC_student_model_class_20.h5\")\nstudent_scenario_3 = tf.keras.models.load_model(\"/kaggle/input/evaluate-model-b/tensorflow2/default/1/NOBIC/5_step_best_NOBIC_student_model_class_20.h5\")\n\n# NoBICPhase1\nstudent_scenario_4 = tf.keras.models.load_model(\"/kaggle/input/evaluate-model-b/tensorflow2/default/1/NOBIC PHASE 1/NOBICPhase1_best_bic_step_2_student_model_epoch_custom_20_class (1).h5\")\nstudent_scenario_5 = tf.keras.models.load_model(\"/kaggle/input/evaluate-model-b/tensorflow2/default/1/NOBIC PHASE 1/NOBICPhase1_best_bic_step_4_student_model_epoch_custom_20_class (5).h5\")\nstudent_scenario_6 = tf.keras.models.load_model(\"/kaggle/input/evaluate-model-b/tensorflow2/default/1/NOBIC PHASE 1/NOBICPhase1_best_bic_step_5_student_model_epoch_custom_20_class (4).h5\")\nbic_scenario_data_4 = np.load('/kaggle/input/evaluate-model-b/tensorflow2/default/1/NOBIC PHASE 1/bic_layer_parameters_2_step_20_class.npz')\nbic_scenario_data_5 = np.load('/kaggle/input/evaluate-model-b/tensorflow2/default/1/NOBIC PHASE 1/bic_layer_parameters_4_step_20_class (1).npz') \nbic_scenario_data_6 = np.load('/kaggle/input/evaluate-model-b/tensorflow2/default/1/NOBIC PHASE 1/bic_layer_parameters_5_step_20_class (2).npz')\n\n# AllBIC\nstudent_scenario_7 = tf.keras.models.load_model(\"/kaggle/input/evaluate-model-b/tensorflow2/default/1/BIC/best_step_2_bic_student_model_epoch_custom_20_class (3).h5\")\nstudent_scenario_8 = tf.keras.models.load_model(\"/kaggle/input/evaluate-model-b/tensorflow2/default/1/BIC/best_step_4_bic_student_model_epoch_custom_20_class (4).h5\")\nstudent_scenario_9 = tf.keras.models.load_model(\"/kaggle/input/evaluate-model-b/tensorflow2/default/1/BIC/best_step_5_bic_student_model_epoch_custom_20_class (3).h5\")\nbic_scenario_data_7 = np.load('/kaggle/input/evaluate-model-b/tensorflow2/default/1/BIC/bic_layer_parameters_2_step_20_class.npz')\nbic_scenario_data_8 = np.load('/kaggle/input/evaluate-model-b/tensorflow2/default/1/BIC/bic_layer_parameters_4_step_20_class.npz') \nbic_scenario_data_9 = np.load('/kaggle/input/evaluate-model-b/tensorflow2/default/1/BIC/bic_layer_parameters_5_step_20_class.npz')\n\nbic_scenario_4, bic_scenario_5, bic_scenario_6 = BiCLayer(18, 2), BiCLayer(16, 4), BiCLayer(15, 5)\nbic_scenario_7, bic_scenario_8, bic_scenario_9 = BiCLayer(18, 2), BiCLayer(16, 4), BiCLayer(15, 5)\n\nbic_scenario_4.alpha.assign(bic_scenario_data_4['alpha'])\nbic_scenario_4.beta.assign(bic_scenario_data_4['beta'])\nbic_scenario_5.alpha.assign(bic_scenario_data_5['alpha'])\nbic_scenario_5.beta.assign(bic_scenario_data_5['beta'])\nbic_scenario_6.alpha.assign(bic_scenario_data_6['alpha'])\nbic_scenario_6.beta.assign(bic_scenario_data_6['beta'])\nbic_scenario_7.alpha.assign(bic_scenario_data_7['alpha'])\nbic_scenario_7.beta.assign(bic_scenario_data_7['beta'])\nbic_scenario_8.alpha.assign(bic_scenario_data_8['alpha'])\nbic_scenario_8.beta.assign(bic_scenario_data_8['beta'])\nbic_scenario_9.alpha.assign(bic_scenario_data_9['alpha'])\nbic_scenario_9.beta.assign(bic_scenario_data_9['beta'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T03:02:18.640538Z","iopub.execute_input":"2025-03-13T03:02:18.640798Z","iopub.status.idle":"2025-03-13T03:02:34.383519Z","shell.execute_reply.started":"2025-03-13T03:02:18.640774Z","shell.execute_reply":"2025-03-13T03:02:34.382611Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"picked_class = True\ndef loading_bic_test_val(end_class,picked_class=False):\n    images_test, labels_test = load_new_image_class_test(end_class,picked_class)\n    test_dataset = tf.data.Dataset.from_tensor_slices((images_test, labels_test))\n    del images_test, labels_test\n    images_val, labels_val = load_new_image_class_validation(end_class,picked_class)\n    validation_dataset = tf.data.Dataset.from_tensor_slices((images_val, labels_val))\n    del images_val, labels_val\n    return test_dataset, validation_dataset\n    \ntest_data, val_data = loading_bic_test_val(20,picked_class)\ntest_data = test_data.batch(32).prefetch(buffer_size=tf.data.AUTOTUNE)\ndataset_test = test_data\ndataset_val = val_data.batch(16)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T03:02:34.384781Z","iopub.execute_input":"2025-03-13T03:02:34.385378Z","iopub.status.idle":"2025-03-13T03:03:04.132485Z","shell.execute_reply.started":"2025-03-13T03:02:34.385336Z","shell.execute_reply":"2025-03-13T03:03:04.131682Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport tensorflow as tf\n\n# Get one batch of images and labels\nfor images, labels in dataset_test.take(1):  # Take one batch\n    images_np = images.numpy()  # Convert images to NumPy array\n    labels_np = labels.numpy()  # Convert labels to NumPy array\n\n    # Plot images\n    plt.figure(figsize=(10, 5))\n    for i in range(min(10, len(images_np))):  # Show up to 10 images\n        plt.subplot(2, 5, i + 1)\n        plt.imshow(images_np[i])  # Assuming images are in standard format (H, W, C)\n        plt.title(f\"Label: {labels_np[i]}\")  # Show label\n        plt.axis(\"off\")\n\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T03:03:04.133712Z","iopub.execute_input":"2025-03-13T03:03:04.134086Z","iopub.status.idle":"2025-03-13T03:03:07.324832Z","shell.execute_reply.started":"2025-03-13T03:03:04.134048Z","shell.execute_reply":"2025-03-13T03:03:07.323897Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report, f1_score, confusion_matrix,accuracy_score\nimport tensorflow as tf\ndef predict_with_bic(student_model, bic_layer, dataset):\n    all_predictions = []\n    all_true_labels = []\n    \n    for x_batch, y_batch in dataset:\n        # Get logits from the student model\n        logits = student_model(x_batch, training=False)\n        \n        # Apply the BiC layer to correct logits\n        corrected_logits = bic_layer(logits)\n        \n        # Apply softmax to get probabilities\n        probabilities = tf.nn.softmax(corrected_logits)\n        # Get predicted classes\n        predicted_classes = tf.argmax(probabilities, axis=-1).numpy()\n        \n        # Collect predictions and true labels\n        all_predictions.extend(predicted_classes)\n        all_true_labels.extend(y_batch.numpy())  # Ensure y_batch is in the correct format\n    \n    return np.array(all_predictions), np.array(all_true_labels)\n\n# # Predict with the model and BiC layer (No BIC Phase 1)\npredicted_scenario_4, true_labels = predict_with_bic(student_scenario_4, bic_scenario_4, dataset_val)\npredicted_scenario_5, true_labels = predict_with_bic(student_scenario_5, bic_scenario_5, dataset_val)\npredicted_scenario_6, true_labels = predict_with_bic(student_scenario_6, bic_scenario_6, dataset_val)\n## All BiC\npredicted_scenario_7, true_labels = predict_with_bic(student_scenario_7, bic_scenario_4, dataset_val)\npredicted_scenario_8, true_labels = predict_with_bic(student_scenario_8, bic_scenario_5, dataset_val)\npredicted_scenario_9, true_labels = predict_with_bic(student_scenario_9, bic_scenario_6, dataset_val)\n# Convert true labels to class indices if they are one-hot encoded\nif true_labels.ndim > 1:  # Check if true_labels is one-hot encoded\n    true_labels = np.argmax(true_labels, axis=1)\n\nif(picked_class):\n    picked_class_folders = ['n01440764', 'n01443537','n01531178','n01537544','n01582220','n01592084','n01601694','n01667778','n01695060','n01698640','n01751748','n01756291','n01770081','n01770393','n01773157','n01774750','n01795545','n01798484','n01806143','n01818515','n01820546','n01824575','n01828970','n01833805','n01843383','n01860187','n01910747','n01914609','n01924916','n01930112','n01950731','n01955084','n01986214','n02006656','n02007558','n02011460','n02013706','n02018207','n02027492','n02037110','n02051845','n02077923']\n    class_folders= picked_class_folders[:20]\nelse:\n    class_folders = sorted(os.listdir(main_directory))[:20]\n\n# Calculate metrics\nfor x in [4,5,6,7,8,9]:\n    predicted_classes = {\n        4: predicted_scenario_4,\n        5: predicted_scenario_5,\n        6: predicted_scenario_6,\n        7: predicted_scenario_7,\n        8: predicted_scenario_8,\n        9: predicted_scenario_9\n    }[x] \n    \n    print(f\"For predicted_classes_{x}\")\n    print(\"Classification Report:\")\n    print(classification_report(true_labels, predicted_classes, target_names=class_folders))\n    \n    # Calculate macro F1-score directly\n    macro_f1 = f1_score(true_labels, predicted_classes, average='macro')\n    print(\"Macro F1-Score:\", macro_f1)\n    \n    macro_accuracy = accuracy_score(true_labels, predicted_classes)\n    print(\"Macro Accuracy:\", macro_accuracy)\n    \n    # Confusion matrix (optional, for deeper analysis)\n    conf_matrix = confusion_matrix(true_labels, predicted_classes)\n    print(\"Confusion Matrix:\")\n    print(conf_matrix)\n\n    print(\"//////////////\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T03:03:07.326177Z","iopub.execute_input":"2025-03-13T03:03:07.32671Z","iopub.status.idle":"2025-03-13T03:04:05.247233Z","shell.execute_reply.started":"2025-03-13T03:03:07.326669Z","shell.execute_reply":"2025-03-13T03:04:05.246346Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Without Training BIC!","metadata":{}},{"cell_type":"code","source":"# Iterate over the validation dataset\nfor student in [2,4,5]:\n    predictions_non_bic = []\n    true_labels_non_bic = []\n    student_model = {\n        2 : student_scenario_1,\n        4 : student_scenario_2,\n        5 : student_scenario_3\n    }[student]\n    for x_batch_val, y_batch_val in dataset_val:\n        # Make predictions\n        preds = student_model.predict(x_batch_val)\n        predicted_classes_non_bic = np.argmax(preds, axis=1)\n                \n        # Collect predictions and true labels\n        predictions_non_bic.extend(predicted_classes_non_bic)\n        true_labels_non_bic.extend(np.argmax(y_batch_val.numpy(), axis=1))  # Assuming y_batch_val is one-hot encoded\n            \n    # Convert lists to numpy arrays\n    predictions = np.array(predictions_non_bic)\n    true_labels = np.array(true_labels_non_bic)\n    \n    # Calculate metrics\n    print(f\"Classification Report for class {student}:\")\n    print(classification_report(true_labels, predictions, target_names=class_folders))\n            \n    # Calculate macro F1-score directly\n    macro_f1 = f1_score(true_labels, predictions, average='macro')\n    print(\"Macro F1-Score:\", macro_f1)\n\n    macro_accuracy = accuracy_score(true_labels, predictions)\n    print(\"Macro Accuracy:\", macro_accuracy)\n    \n    # Confusion matrix (optional, for deeper analysis)\n    conf_matrix = confusion_matrix(true_labels, predictions)\n    print(\"Confusion Matrix:\")\n    print(conf_matrix)\n\n    print(\"//////////////\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T07:21:19.546703Z","iopub.execute_input":"2025-02-17T07:21:19.547319Z","iopub.status.idle":"2025-02-17T07:21:50.20777Z","shell.execute_reply.started":"2025-02-17T07:21:19.547284Z","shell.execute_reply":"2025-02-17T07:21:50.206938Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Real BIC","metadata":{}},{"cell_type":"code","source":"# # Step 3: Create the BiC layer model\n# num_old_classes = 10  # Adjust according to your setup\n# num_new_classes = 5  # Adjust according to your setup\n# bic_layer = BiCLayer(num_old_classes, num_new_classes)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# validation_bic = dataset_test.unbatch()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import tensorflow as tf\n# from collections import defaultdict\n\n# # Create a dictionary to store data and labels for each class\n# data_by_class = defaultdict(list)\n\n# # Unbatch the dataset\n# for data, label in dataset_test.unbatch():  # Replace `32` with your batch size if known\n#     label_value = tf.argmax(label).numpy() if len(label.shape) > 0 and label.shape[-1] > 1 else label.numpy()\n#     data_by_class[label_value].append((data.numpy(), label_value))\n\n# # Print the number of samples for each class\n# for class_label, samples in data_by_class.items():\n#     print(f\"Class {class_label} has {len(samples)} samples.\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import tensorflow as tf\n# import numpy as np\n\n# # Combine the data and labels from all classes into a single list\n# combined_data = []\n# for class_label, samples in data_by_class.items():\n#     combined_data.extend(samples)  # Each `samples` is a list of (data, label) tuples\n\n# # Shuffle the combined data\n# np.random.shuffle(combined_data)\n\n# # Separate features and labels\n# features = np.array([sample[0] for sample in combined_data])\n# labels = np.array([sample[1] for sample in combined_data])\n\n# # Create a tf.data.Dataset and batch it with a batch size of 32\n# batched_dataset = tf.data.Dataset.from_tensor_slices((features, labels)).batch(32)\n\n# # Print the number of batches\n# print(f\"The dataset has been batched into {len(list(batched_dataset))} batches.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Early stopping parameters\npatience = 15  # Number of epochs to wait before stopping if no improvement\nmin_delta = 0.01  # Minimum change in the monitored metric to qualify as an improvement\nbest_val_loss = np.inf\nepochs_without_improvement = 0\ntrain_loss_bic = tf.keras.metrics.Mean(name='train_loss')\ntrain_accuracy_bic = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\noptimizer = keras.optimizers.Adam(learning_rate=0.01)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @tf.function\n# def training_bic_model(student_model,bic_layer, x_batch_train, y_batch_train):\n#     with tf.GradientTape() as tape:\n#         # Forward pass for student model\n#         logits = student_model(x_batch_train, training=True)\n#         corrected_logits = bic_layer(logits)  # Apply BiC layer\n#         loss = tf.keras.losses.categorical_crossentropy(y_batch_train, corrected_logits, from_logits=True)\n\n#     # Update gradients only for the BiC layer parameters\n#     gradients = tape.gradient(loss, [bic_layer.alpha, bic_layer.beta])\n#     optimizer.apply_gradients(zip(gradients, [bic_layer.alpha, bic_layer.beta]))\n\n#     # Update training metrics\n#     train_loss_bic.update_state(loss)\n#     train_accuracy_bic.update_state(y_batch_train, tf.nn.softmax(corrected_logits))\n\n#     return loss","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Step 4: Apply BiC layer on logits generated by the student model\n# for epoch in range(epochs):\n#     print(f\"\\nStart of epoch {epoch}\")\n\n#     # Reset training metrics\n#     train_loss_bic.reset_state()\n#     train_accuracy_bic.reset_state()\n\n#     for step, (x_batch_train, y_batch_train) in enumerate(batched_dataset):\n#         y_batch_train = tf.one_hot(y_batch_train, depth=15)\n#         loss_bic = training_bic_model(student_model,bic_layer,x_batch_train, y_batch_train)\n\n#         if step % 100 == 0:\n#             print(f\"Step {step}: Loss = {train_loss_bic.result():.4f}, Accuracy = {train_accuracy_bic.result() * 100:.2f}%\")\n\n#     # Print training summary for the epoch\n#     print(f\"Epoch {epoch}: Training Loss = {train_loss_bic.result():.4f}, Training Accuracy = {train_accuracy_bic.result() * 100:.2f}%\")\n\n#     # Validation and early stopping code (similar to previous implementation)\n#     current_val_loss = train_loss_bic.result()\n#     if current_val_loss < (best_val_loss - min_delta):\n#         best_val_loss = current_val_loss\n#         epochs_without_improvement = 0\n#         # Optionally, save the model when a new best is found\n#         student_model.save(f'{save_dir}/best_bic_model_epoch_{epoch}.h5')\n#     else:\n#         epochs_without_improvement += 1\n\n#     if epochs_without_improvement >= patience:\n#         print(f\"Early stopping triggered after {epoch + 1} epochs due to no improvement.\")\n#         break\n\n#         # Get logits from the student model\n#         logits = student_model(x_batch, training=False)\n        \n#         # Apply the BiC layer to correct logits\n#         corrected_logits = bic_layer(logits)\n        \n#         # Apply softmax to get probabilities\n#         probabilities = tf.nn.softmax(corrected_logits)\n        \n#         # Get predicted classes\n#         predicted_classes = tf.argmax(probabilities, axis=-1).numpy()\n        \n#         # Collect predictions and true labels\n#         all_predictions.extend(predicted_classes)\n#         all_true_labels.extend(y_batch.numpy())  # Ensure y_batch is in the correct format\n    \n#     # return np.array(all_predictions), np.array(all_true_labels)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Assuming bic_layer is an instance of BiCLayer\n# alpha_value = bic_layer.alpha.numpy()\n\n# print(f\"Alpha value: {alpha_value} Beta Value Beta Value : {bic_layer.beta.numpy()}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Function to predict using the student model and BiC layer\n# def predict_with_bic(student_model, bic_layer, input_data):\n#     # Get logits from the student model\n#     logits = student_model(input_data, training=False)\n    \n#     # Apply the BiC layer to adjust the logits\n#     corrected_logits = bic_layer(logits)\n    \n#     # Apply softmax to get probabilities\n#     probabilities = tf.nn.softmax(corrected_logits)\n    \n#     # Get the predicted class\n#     predicted_classes = tf.argmax(probabilities, axis=-1)\n    \n#     return predicted_classes, probabilities","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import numpy as np\n# from sklearn.metrics import classification_report, f1_score, confusion_matrix\n# import tensorflow as tf\n\n# # Function to get predictions using the BiC layer\n# def predict_with_bic(student_model, bic_layer, dataset):\n#     all_predictions = []\n#     all_true_labels = []\n    \n#     for x_batch, y_batch in dataset:\n#         # Get logits from the student model\n#         logits = student_model(x_batch, training=False)\n        \n#         # Apply the BiC layer to correct logits\n#         corrected_logits = bic_layer(logits)\n        \n#         # Apply softmax to get probabilities\n#         probabilities = tf.nn.softmax(corrected_logits)\n        \n#         # Get predicted classes\n#         predicted_classes = tf.argmax(probabilities, axis=-1).numpy()\n        \n#         # Collect predictions and true labels\n#         all_predictions.extend(predicted_classes)\n#         all_true_labels.extend(y_batch.numpy())  # Ensure y_batch is in the correct format\n    \n#     return np.array(all_predictions), np.array(all_true_labels)\n\n# # Predict with the model and BiC layer\n# predicted_classes, true_labels = predict_with_bic(student_model, bic_layer, dataset_val)\n\n# # Convert true labels to class indices if they are one-hot encoded\n# if true_labels.ndim > 1:  # Check if true_labels is one-hot encoded\n#     true_labels = np.argmax(true_labels, axis=1)\n\n# # Calculate metrics\n# print(\"Classification Report:\")\n# print(classification_report(true_labels, predicted_classes, target_names=class_folders))\n\n# # Calculate macro F1-score directly\n# macro_f1 = f1_score(true_labels, predicted_classes, average='macro')\n# print(\"Macro F1-Score:\", macro_f1)\n\n# # Confusion matrix (optional, for deeper analysis)\n# conf_matrix = confusion_matrix(true_labels, predicted_classes)\n# print(\"Confusion Matrix:\")\n# print(conf_matrix)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}